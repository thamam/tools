# Voice Transcription Tool v2.1 Implementation Plan

**Target Timeline:** 10 working days  
**Goal:** Improve performance, enhance UX, fix technical debt

---

## Priority 1: Performance Improvements (3 days)

### P1.1: GPU Detection & Acceleration (1 day)

**Objective:** Enable GPU acceleration when available, maintain CPU fallback

**Technical Details:**
- Detect CUDA/ROCm on startup using `torch.cuda.is_available()`
- Enable `fp16=True` automatically for GPU inference
- Add config option `force_cpu` for debugging
- Add startup log: "GPU detected: Tesla V100 (16GB)" or "Running on CPU"

**Files to Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ speech/engines.py
â”‚   â”œâ”€â”€ WhisperEngine.__init__() - add gpu_enabled parameter
â”‚   â”œâ”€â”€ WhisperEngine._load_model() - detect GPU, set fp16
â”‚   â””â”€â”€ Add _detect_gpu() helper method
â”œâ”€â”€ config/settings.py
â”‚   â””â”€â”€ Add 'force_cpu': False to defaults
â””â”€â”€ main.py
    â””â”€â”€ Add GPU info to startup logs
```

**Implementation Steps:**
1. Add GPU detection helper:
```python
def _detect_gpu(self) -> tuple[bool, str]:
    """Returns (has_gpu, device_name)"""
    try:
        import torch
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            return (True, device_name)
    except:
        pass
    return (False, "CPU")
```

2. Update WhisperEngine initialization:
```python
self.has_gpu, self.device = self._detect_gpu()
force_cpu = config.get('force_cpu', False)
self.use_gpu = self.has_gpu and not force_cpu
```

3. Update transcribe() to use GPU settings:
```python
result = self.model.transcribe(
    audio_file,
    fp16=self.use_gpu,  # Auto-detect instead of hardcoded False
    device="cuda" if self.use_gpu else "cpu",
    # ... rest of settings
)
```

**Testing:**
- Test on machine with GPU (verify CUDA usage with `nvidia-smi`)
- Test on CPU-only machine (verify fallback)
- Test with `force_cpu=True` config option

---

### P1.2: Faster-Whisper Integration (1.5 days)

**Objective:** 4x speedup using CTranslate2-optimized Whisper

**Technical Details:**
- Add `faster-whisper>=0.10.0` dependency
- Create `FasterWhisperEngine` class (parallel to `WhisperEngine`)
- Make it default engine with fallback to original
- Compatible API - no changes to calling code

**Files to Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Add: faster-whisper>=0.10.0
â”œâ”€â”€ speech/engines.py
â”‚   â”œâ”€â”€ Add FasterWhisperEngine class (new ~150 lines)
â”‚   â”œâ”€â”€ Update SpeechEngineManager._init_engines()
â”‚   â””â”€â”€ Add engine priority: faster-whisper > whisper > google
â””â”€â”€ tests/test_speech.py
    â””â”€â”€ Add FasterWhisperEngine tests
```

**Implementation:**

```python
class FasterWhisperEngine(SpeechEngine):
    """CTranslate2-optimized Whisper (4x faster)"""
    
    def __init__(self, model_size: str = "base", device: str = "auto"):
        super().__init__()
        self.model_size = model_size
        self.model = None
        self.device = device
        self._load_model()
    
    def _load_model(self):
        try:
            from faster_whisper import WhisperModel
            
            # Auto-detect device
            if self.device == "auto":
                import torch
                device = "cuda" if torch.cuda.is_available() else "cpu"
            else:
                device = self.device
            
            self.model = WhisperModel(
                self.model_size,
                device=device,
                compute_type="float16" if device == "cuda" else "int8"
            )
            self.logger.info(f"âœ… Faster-Whisper loaded: {self.model_size} on {device}")
        except ImportError:
            self.logger.warning("faster-whisper not installed, falling back")
        except Exception as e:
            self.logger.error(f"Failed to load faster-whisper: {e}")
    
    def transcribe(self, audio_file: str) -> dict:
        if not self.model:
            return {'success': False, 'error': 'Model not loaded'}
        
        try:
            segments, info = self.model.transcribe(
                audio_file,
                beam_size=1,
                best_of=1,
                temperature=0.0,
                vad_filter=True,  # Voice activity detection
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            # Combine segments
            text = " ".join([seg.text for seg in segments]).strip()
            
            return {
                'text': text,
                'confidence': 0.95,
                'method': 'faster-whisper',
                'language': info.language,
                'success': True
            }
        except Exception as e:
            self.logger.error(f"Faster-Whisper transcription failed: {e}")
            return {'success': False, 'error': str(e)}
```

**Engine Priority Order:**
1. faster-whisper (if available)
2. whisper (if available)
3. google (fallback)

**Testing:**
- Benchmark: Record 30s audio, measure transcription time
- Compare accuracy: Same audio through both engines
- Test fallback: Uninstall faster-whisper, verify whisper still works

---

### P1.3: Model Size Selector UI (0.5 days)

**Objective:** Let users choose Whisper model size (speed vs accuracy trade-off)

**Technical Details:**
- Add dropdown to settings: tiny/base/small/medium/large
- Save to config as `whisper_model_size`
- Reload model when changed (with progress dialog)
- Show estimated speed/quality in tooltip

**Files to Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ config/settings.py
â”‚   â””â”€â”€ Add 'whisper_model_size': 'base' to defaults
â””â”€â”€ gui/main_window.py
    â””â”€â”€ Update _open_settings() method
```

**Implementation:**

Add to settings dialog:
```python
# Model size selector
model_frame = ttk.LabelFrame(settings_window, text="Whisper Model", padding=10)
model_frame.pack(fill="x", padx=10, pady=5)

ttk.Label(model_frame, text="Model Size:").pack(side="left")

model_var = tk.StringVar(value=self.config.get('whisper_model_size', 'base'))
model_dropdown = ttk.Combobox(
    model_frame, 
    textvariable=model_var,
    values=['tiny', 'base', 'small', 'medium', 'large'],
    state='readonly',
    width=15
)
model_dropdown.pack(side="left", padx=10)

# Tooltip
ttk.Label(model_frame, 
    text="tiny=fastest, large=best quality",
    font=("Arial", 9),
    foreground="gray"
).pack(side="left")
```

Model reload logic:
```python
def _apply_model_change(self, new_model_size):
    """Reload Whisper model with progress indicator"""
    if new_model_size == self.speech_manager.whisper_engine.model_size:
        return
    
    # Show progress dialog
    progress = tk.Toplevel(self.root)
    progress.title("Loading Model")
    ttk.Label(progress, text=f"Loading {new_model_size} model...").pack(padx=20, pady=20)
    progress_bar = ttk.Progressbar(progress, mode='indeterminate')
    progress_bar.pack(padx=20, pady=10)
    progress_bar.start()
    
    # Reload in thread
    def reload():
        self.speech_manager.reload_whisper_model(new_model_size)
        self.root.after(0, progress.destroy)
    
    threading.Thread(target=reload, daemon=True).start()
```

**Testing:**
- Select each model size, verify it loads
- Check config persistence across restarts
- Test cancel during model load

---

## Priority 2: UX Polish (3 days)

### P2.1: Visual Recording Feedback (1 day)

**Objective:** Clear visual indicators when recording is active

**Components:**
1. **Recording Banner** - Pulsing red bar at top
2. **Audio Level Meter** - Green/yellow/red bar showing mic input
3. **Enhanced Status** - Larger, more visible

**Files to Modify:**
```
voice_transcription_tool/gui/main_window.py
â””â”€â”€ _create_gui() - add recording banner
â””â”€â”€ _update_progress_bar() - add level meter
â””â”€â”€ _start_recording() - show banner
â””â”€â”€ _stop_recording() - hide banner
```

**Implementation:**

```python
# In _create_gui()
# Recording banner (hidden by default)
self.recording_banner = ttk.Frame(main_frame, style="Recording.TFrame")
self.recording_banner_label = ttk.Label(
    self.recording_banner,
    text="ðŸ”´ RECORDING IN PROGRESS",
    font=("Arial", 14, "bold"),
    foreground="white",
    background="#dc3545",
    padding=10
)
self.recording_banner_label.pack(fill="x")
# Start hidden
self.recording_banner.pack_forget()

# Add custom style
style = ttk.Style()
style.configure("Recording.TFrame", background="#dc3545")

# Audio level meter
self.level_meter_frame = ttk.Frame(main_frame)
self.level_meter_label = ttk.Label(self.level_meter_frame, text="Input Level:")
self.level_meter_label.pack(side="left")
self.level_meter = ttk.Progressbar(
    self.level_meter_frame,
    mode='determinate',
    length=300,
    maximum=10000  # RMS scale
)
self.level_meter.pack(side="left", padx=10)
self.level_meter_frame.pack(pady=5)
```

**Level Meter Color Logic:**
```python
def _update_level_meter(self, rms: float):
    """Update audio level meter with color coding"""
    self.level_meter['value'] = min(rms, 10000)
    
    # Color zones
    if rms < 500:
        # Too quiet - yellow warning
        self.level_meter.configure(style="Yellow.Horizontal.TProgressbar")
    elif rms > 7000:
        # Too loud - red warning
        self.level_meter.configure(style="Red.Horizontal.TProgressbar")
    else:
        # Good level - green
        self.level_meter.configure(style="Green.Horizontal.TProgressbar")
```

**Pulsing Animation:**
```python
def _pulse_recording_banner(self):
    """Animate recording banner"""
    if not self.is_recording:
        return
    
    # Toggle between bright and dim red
    colors = ["#dc3545", "#8b0000"]
    current = self.recording_banner_label.cget("background")
    next_color = colors[0] if current == colors[1] else colors[1]
    self.recording_banner_label.configure(background=next_color)
    
    # Schedule next pulse
    self.root.after(500, self._pulse_recording_banner)
```

**Testing:**
- Record with normal voice - verify green meter
- Whisper softly - verify yellow warning
- Shout - verify red warning
- Check banner visibility and pulsing

---

### P2.2: System Tray Icon (1.5 days)

**Objective:** Run in background with tray menu

**Features:**
- Tray icon (mic symbol)
- Menu: Show/Hide, Start Recording, Quit
- Recording animation (icon changes to red when recording)
- Notification on transcription complete

**Files to Create/Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ utils/tray_manager.py (NEW - ~250 lines)
â”œâ”€â”€ gui/main_window.py
â”‚   â””â”€â”€ Integrate TrayManager in __init__
â”‚   â””â”€â”€ Add _hide_to_tray() method
â””â”€â”€ requirements.txt
    â””â”€â”€ Add: pystray>=0.19.0, pillow>=9.0.0
```

**Implementation:**

```python
# utils/tray_manager.py
from pystray import Icon, Menu, MenuItem
from PIL import Image, ImageDraw

class TrayManager:
    """System tray icon manager"""
    
    def __init__(self, app):
        self.app = app
        self.icon = None
        self.is_recording = False
        
    def create_icon(self, color="black"):
        """Generate microphone icon programmatically"""
        img = Image.new('RGB', (64, 64), color='white')
        draw = ImageDraw.Draw(img)
        
        # Draw microphone shape
        if color == "red":
            fill = "#dc3545"
        else:
            fill = "#333333"
        
        # Mic body (rounded rectangle)
        draw.ellipse([20, 10, 44, 40], fill=fill)
        # Mic stand
        draw.rectangle([30, 40, 34, 55], fill=fill)
        draw.ellipse([25, 50, 39, 58], fill=fill)
        
        return img
    
    def create_menu(self):
        """Create tray menu"""
        return Menu(
            MenuItem(
                'Show Window',
                lambda: self.app.root.deiconify(),
                default=True
            ),
            MenuItem(
                'Start Recording',
                lambda: self.app._toggle_recording(),
                enabled=lambda item: not self.is_recording
            ),
            MenuItem(
                'Stop Recording',
                lambda: self.app._toggle_recording(),
                enabled=lambda item: self.is_recording
            ),
            Menu.SEPARATOR,
            MenuItem('Quit', lambda: self.app._on_closing())
        )
    
    def start(self):
        """Start tray icon"""
        self.icon = Icon(
            "Voice Transcription",
            self.create_icon(),
            "Voice Transcription Tool",
            self.create_menu()
        )
        # Run in separate thread
        threading.Thread(target=self.icon.run, daemon=True).start()
    
    def set_recording_state(self, is_recording: bool):
        """Update icon when recording state changes"""
        self.is_recording = is_recording
        if self.icon:
            color = "red" if is_recording else "black"
            self.icon.icon = self.create_icon(color)
    
    def show_notification(self, title: str, message: str):
        """Show desktop notification"""
        if self.icon:
            self.icon.notify(message, title)
```

**Integration in main_window.py:**
```python
# In __init__
from utils.tray_manager import TrayManager
self.tray_manager = TrayManager(self)
self.tray_manager.start()

# In _start_recording
self.tray_manager.set_recording_state(True)

# In _stop_recording  
self.tray_manager.set_recording_state(False)

# After transcription
self.tray_manager.show_notification(
    "Transcription Complete",
    f"Text: {result['text'][:50]}..."
)
```

**Testing:**
- Verify tray icon appears
- Test menu items (show/hide, record, quit)
- Check recording animation (black â†’ red)
- Test notification on transcription complete
- Test minimize to tray behavior

---

### P2.3: Keyboard Shortcuts (0.5 days)

**Objective:** Common actions accessible via keyboard

**Shortcuts:**
- `Ctrl+R` - Toggle recording
- `Ctrl+C` - Copy transcription to clipboard
- `Ctrl+Q` - Quit application
- `Esc` - Stop recording (if active)

**Files to Modify:**
```
voice_transcription_tool/gui/main_window.py
â””â”€â”€ _create_gui() - bind keyboard events
â””â”€â”€ _handle_keyboard_shortcut() (NEW method)
```

**Implementation:**

```python
def _setup_keyboard_shortcuts(self):
    """Bind keyboard shortcuts"""
    self.root.bind('<Control-r>', lambda e: self._toggle_recording())
    self.root.bind('<Control-R>', lambda e: self._toggle_recording())
    self.root.bind('<Control-c>', lambda e: self._copy_to_clipboard())
    self.root.bind('<Control-C>', lambda e: self._copy_to_clipboard())
    self.root.bind('<Control-q>', lambda e: self._on_closing())
    self.root.bind('<Control-Q>', lambda e: self._on_closing())
    self.root.bind('<Escape>', lambda e: self._handle_escape())
    
    self.logger.info("Keyboard shortcuts enabled")

def _handle_escape(self):
    """Handle Escape key - stop recording if active"""
    if self.is_recording:
        self._stop_recording()
        self.logger.info("Recording stopped via Escape key")

# In _create_gui, after creating root window:
self._setup_keyboard_shortcuts()
```

**Add shortcuts help to status bar:**
```python
shortcuts_label = ttk.Label(
    main_frame,
    text="Shortcuts: Ctrl+R (record) | Ctrl+C (copy) | Ctrl+Q (quit) | Esc (stop)",
    font=("Arial", 8),
    foreground="gray"
)
shortcuts_label.pack(side="bottom", pady=5)
```

**Testing:**
- Test each shortcut key combination
- Verify Escape only works when recording
- Test with focus on different UI elements

---

## Priority 3: Fixes & Cleanup (1 day)

### P3.1: Better Error Messages (2 hours)

**Objective:** User-friendly error messages with actionable solutions

**Current Issues:**
- Generic "Transcription failed" messages
- No guidance on fixing problems
- Technical jargon in user-facing errors

**Files to Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ speech/engines.py - improve error context
â””â”€â”€ audio/recorder.py - device-specific errors
```

**Error Message Template:**
```python
ERROR_MESSAGES = {
    'gpu_not_available': {
        'title': 'GPU Not Available',
        'message': 'No NVIDIA GPU detected. Using CPU (slower).\n\n'
                   'To enable GPU acceleration:\n'
                   '1. Install CUDA Toolkit\n'
                   '2. Install PyTorch with CUDA support\n'
                   '3. Restart application',
        'severity': 'warning'
    },
    'model_load_failed': {
        'title': 'Model Load Failed',
        'message': 'Failed to load Whisper model "{model_size}".\n\n'
                   'Possible fixes:\n'
                   '1. Check internet connection (first-time download)\n'
                   '2. Try smaller model (Settings â†’ Model Size)\n'
                   '3. Clear model cache: rm -rf ~/.cache/whisper',
        'severity': 'error'
    },
    'audio_device_error': {
        'title': 'Microphone Error',
        'message': 'Cannot access microphone.\n\n'
                   'Please check:\n'
                   '1. Microphone is connected\n'
                   '2. Permissions granted (Settings â†’ Privacy)\n'
                   '3. No other app is using the microphone',
        'severity': 'error'
    }
}
```

**Testing:**
- Trigger each error condition
- Verify user-friendly message appears
- Test actionable instructions work

---

### P3.2: Remove Dead Tray Stub (1 hour)

**Objective:** Clean up non-functional system tray code

**Actions:**
- Delete `utils/system_tray.py` (stub file)
- Remove references from `tests/test_utils.py`
- Clean up any imports

**Files to Delete/Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ utils/system_tray.py (DELETE)
â””â”€â”€ tests/test_utils.py (remove stub tests)
```

**Testing:**
- Run full test suite - verify no broken imports
- Check for any lingering references with grep

---

### P3.3: Optimize RMS Calculation (1 hour)

**Objective:** Reduce CPU usage during recording

**Current Issue:**
- RMS calculated every audio chunk (~10ms)
- Only displayed every 100ms
- Wasting 90% of calculations

**Files to Modify:**
```
voice_transcription_tool/
â”œâ”€â”€ audio/recorder.py
â”‚   â””â”€â”€ Add _rms_cache and throttling
â””â”€â”€ gui/main_window.py
    â””â”€â”€ Use cached RMS values
```

**Implementation:**

```python
# In AudioRecorder class
def __init__(self, ...):
    # ... existing code ...
    self._rms_cache = 0.0
    self._last_rms_time = 0

def _calculate_rms_throttled(self, audio_data: bytes) -> float:
    """Calculate RMS with 100ms throttling"""
    current_time = time.time()
    
    # Only recalculate every 100ms
    if current_time - self._last_rms_time < 0.1:
        return self._rms_cache
    
    self._rms_cache = self._calculate_rms(audio_data)
    self._last_rms_time = current_time
    return self._rms_cache
```

**Expected Result:**
- 90% reduction in RMS calculation overhead
- No visible change in UI responsiveness

**Testing:**
- Monitor CPU usage before/after
- Verify level meter still updates smoothly

---

## Testing & Validation (1 day)

### Unit Tests
- Add tests for GPU detection
- Test FasterWhisperEngine
- Test tray manager icon generation
- Test keyboard shortcut handlers

### Integration Tests
- Full recording â†’ transcription workflow with GPU
- Model size changes persist across restarts
- Tray icon integrates with main window

### Performance Tests
- Benchmark transcription times (before/after)
- Stress test: 100 consecutive recordings
- Memory leak check with new features

### User Acceptance Testing
- Record 10 different audio clips
- Test all new UI features
- Verify system tray works on minimized state

---

## Deployment Checklist

- [ ] Update `requirements.txt` with new dependencies
- [ ] Update WARP.md with v2.1 features
- [ ] Create CHANGELOG.md entry
- [ ] Tag release: `git tag v2.1.0`
- [ ] Test on clean Ubuntu install
- [ ] Update README with new features
- [ ] Create migration guide from v2.0

---

## Rollback Plan

If critical issues arise:
1. Faster-whisper can be disabled via config
2. Tray icon is optional (app works without it)
3. GPU acceleration auto-falls back to CPU
4. Model size can be changed back to "tiny"

Each feature is independent and can be rolled back individually.

---

## Success Metrics

**Performance:**
- [ ] Transcription time reduced by 50%+ (with GPU/faster-whisper)
- [ ] CPU usage during recording < 30%
- [ ] Memory usage stable (< 2GB)

**UX:**
- [ ] User can identify recording state within 1 second
- [ ] All keyboard shortcuts work reliably
- [ ] System tray menu responsive (< 100ms)

**Quality:**
- [ ] All tests pass (114 existing + ~20 new)
- [ ] Test coverage remains > 70%
- [ ] Zero P0 bugs in manual testing

---

## Day-by-Day Schedule

**Day 1:** P1.1 GPU detection + P1.2 faster-whisper (start)  
**Day 2:** P1.2 faster-whisper (finish) + P1.3 model selector  
**Day 3:** Testing Priority 1 features  
**Day 4:** P2.1 visual feedback + P2.2 tray icon (start)  
**Day 5:** P2.2 tray icon (finish) + P2.3 keyboard shortcuts  
**Day 6:** Testing Priority 2 features  
**Day 7:** P3.1 error messages + P3.2 cleanup + P3.3 optimize  
**Day 8:** Full integration testing  
**Day 9:** Bug fixes and polish  
**Day 10:** Final testing, documentation, release prep

---

**Ready to start implementation?** I recommend beginning with P1.1 (GPU detection) as it's the foundation for performance improvements.
